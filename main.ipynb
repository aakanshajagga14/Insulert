import os
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score

from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.preprocessing.image import ImageDataGenerator


# -------------------------------------------------------------------
# Config & reproducibility
# -------------------------------------------------------------------
SEED = 42
DATA_DIR = "../input/prepossessed-arrays-of-binary-data"
INFO_CSV = os.path.join(DATA_DIR, "1000_Binary Dataframe")
BATCH_SIZE = 16
EPOCHS = 30
VAL_SPLIT = 0.2
USE_AUGMENTATION = True  # toggle this if augmentation corrupts your patterns

np.random.seed(SEED)
tf.random.set_seed(SEED)


# -------------------------------------------------------------------
# Data loading
# -------------------------------------------------------------------
def load_metadata(info_csv: str):
    """Load dataframe and labels from CSV."""
    info = pd.read_csv(info_csv)
    if "Unnamed: 0" in info.columns:
        info = info.drop(columns=["Unnamed: 0"])
    labels = info["level"].astype("int32").values
    n_classes = labels.max() + 1
    return info, labels, n_classes


def load_binary_images(size: int, data_dir: str) -> np.ndarray:
    """Load preprocessed binary images for a given resolution."""
    file_map = {
        90: "1000_Binary_images_data_90.npz",
        128: "1000_Binary_images_data_128.npz",
        264: "1000_Binary_images_data_264.npz",
    }
    path = os.path.join(data_dir, file_map[size])
    data = np.load(path)["a"].astype("float32") / 255.0
    return data.reshape(-1, size, size, 3)


# -------------------------------------------------------------------
# Model
# -------------------------------------------------------------------
def build_cnn(input_shape, n_classes: int) -> tf.keras.Model:
    """Build a CNN classifier."""
    inputs = layers.Input(shape=input_shape)

    x = layers.Conv2D(32, 3, padding="same", activation="relu")(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D(2)(x)

    x = layers.Conv2D(64, 3, padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D(2)(x)

    x = layers.Conv2D(128, 3, padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D(2)(x)

    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dropout(0.5)(x)

    # optional extra dense layer for more capacity
    x = layers.Dense(128, activation="relu")(x)
    x = layers.Dropout(0.5)(x)

    outputs = layers.Dense(n_classes, activation="softmax")(x)

    model = models.Model(inputs, outputs)
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"],
    )
    return model


# -------------------------------------------------------------------
# Callbacks
# -------------------------------------------------------------------
def build_callbacks(size: int):
    """Build standard training callbacks."""
    checkpoint_path = f"best_cnn_{size}.keras"

    checkpoint_cb = callbacks.ModelCheckpoint(
        checkpoint_path,
        monitor="val_loss",
        save_best_only=True,
        verbose=1,
    )

    earlystop_cb = callbacks.EarlyStopping(
        monitor="val_loss",
        patience=8,
        restore_best_weights=True,
    )

    reducelr_cb = callbacks.ReduceLROnPlateau(
        monitor="val_loss",
        factor=0.5,
        patience=3,
        verbose=1,
    )

    return [checkpoint_cb, earlystop_cb, reducelr_cb]


# -------------------------------------------------------------------
# Training & evaluation
# -------------------------------------------------------------------
def train_and_evaluate(size: int, labels, n_classes: int):
    print(f"
===== Resolution: {size}x{size} =====")

    images = load_binary_images(size, DATA_DIR)
    x_train, x_val, y_train, y_val = train_test_split(
        images,
        labels,
        test_size=VAL_SPLIT,
        random_state=SEED,
        stratify=labels,
    )

    model = build_cnn(input_shape=x_train.shape[1:], n_classes=n_classes)
    model.summary()

    cb_list = build_callbacks(size)

    # optional data augmentation
    if USE_AUGMENTATION:
        datagen = ImageDataGenerator(
            rotation_range=5,
            width_shift_range=0.02,
            height_shift_range=0.02,
            horizontal_flip=True,
        )
        datagen.fit(x_train)
        train_data = datagen.flow(x_train, y_train, batch_size=BATCH_SIZE)
        steps_per_epoch = len(x_train) // BATCH_SIZE
        fit_kwargs = dict(
            x=train_data,
            steps_per_epoch=steps_per_epoch,
        )
    else:
        fit_kwargs = dict(
            x=x_train,
            y=y_train,
            batch_size=BATCH_SIZE,
        )

    start_time = time.time()
    history = model.fit(
        validation_data=(x_val, y_val),
        epochs=EPOCHS,
        callbacks=cb_list,
        verbose=1,
        **fit_kwargs,
    )
    train_time = time.time() - start_time

    val_loss, val_acc = model.evaluate(x_val, y_val, verbose=0)
    print(f"Val loss: {val_loss:.4f} | Val acc: {val_acc:.4f}")
    print(f"Train time: {train_time:.1f}s")

    y_prob = model.predict(x_val, batch_size=BATCH_SIZE)
    y_pred = np.argmax(y_prob, axis=1)

    print("
Classification report:")
    print(classification_report(y_val, y_pred))

    cm = confusion_matrix(y_val, y_pred)
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
    plt.title(f"Confusion matrix ({size}x{size})")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.tight_layout()
    plt.show()

    try:
        auc_macro = roc_auc_score(y_val, y_prob, multi_class="ovr")
        print(f"Macro ROC-AUC: {auc_macro:.4f}")
    except ValueError:
        pass

    return {
        "size": size,
        "val_acc": float(val_acc),
        "val_loss": float(val_loss),
        "train_time": float(train_time),
        "history": history.history,
    }


# -------------------------------------------------------------------
# Main
# -------------------------------------------------------------------
if __name__ == "__main__":
    metadata, labels, n_classes = load_metadata(INFO_CSV)

    results = []
    for resolution in (90, 128, 264):
        result = train_and_evaluate(resolution, labels, n_classes)
        # history is not JSONâ€‘serializable by default; drop it from the summary row
        results.append(
            {
                "size": result["size"],
                "val_acc": result["val_acc"],
                "val_loss": result["val_loss"],
                "train_time": result["train_time"],
            }
        )

    # log summary
    print("
Summary:")
    for r in results:
        print(
            f"{r['size']}x{r['size']} -> "
            f"val_acc={r['val_acc']:.4f}, "
            f"val_loss={r['val_loss']:.4f}, "
            f"time={r['train_time']:.1f}s"
        )

    results_df = pd.DataFrame(results)
    results_df.to_csv("cnn_resolution_results.csv", index=False)
    print("
Saved results to cnn_resolution_results.csv")
