"""
Multi-resolution CNN classifier for binary image data.
Compares 90x90, 128x128, 264x264 resolutions.
"""
import os
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score
from tensorflow.keras import layers, models, callbacks


# =============================================================================
# CONFIG
# =============================================================================
SEED = 42
DATA_DIR = "../input/prepossessed-arrays-of-binary-data"
INFO_CSV = os.path.join(DATA_DIR, "1000_Binary Dataframe")
RESOLUTIONS = [90, 128, 264]
BATCH_SIZE = 16
EPOCHS = 30
VAL_SPLIT = 0.2

np.random.seed(SEED)
tf.random.set_seed(SEED)


# =============================================================================
# DATA
# =============================================================================
def load_labels(csv_path):
    """Load labels from CSV."""
    df = pd.read_csv(csv_path)
    if "Unnamed: 0" in df.columns:
        df = df.drop(columns=["Unnamed: 0"])
    return df["level"].astype("int32").values


def load_images(size):
    """Load images for given size."""
    file_map = {
        90: "1000_Binary_images_data_90.npz",
        128: "1000_Binary_images_data_128.npz",
        264: "1000_Binary_images_data_264.npz",
    }
    path = os.path.join(DATA_DIR, file_map[size])
    data = np.load(path)["a"].astype("float32") / 255.0
    return data.reshape(-1, size, size, 3)


def prepare_data(images, labels):
    """Split data with stratification."""
    return train_test_split(
        images, labels, test_size=VAL_SPLIT, random_state=SEED, stratify=labels
    )


# =============================================================================
# MODEL
# =============================================================================
def create_model(input_shape, n_classes):
    """Create CNN classifier."""
    model = models.Sequential([
        layers.Conv2D(32, 3, padding="same", activation="relu", input_shape=input_shape),
        layers.BatchNormalization(),
        layers.MaxPooling2D(2),

        layers.Conv2D(64, 3, padding="same", activation="relu"),
        layers.BatchNormalization(),
        layers.MaxPooling2D(2),

        layers.Conv2D(128, 3, padding="same", activation="relu"),
        layers.BatchNormalization(),
        layers.MaxPooling2D(2),

        layers.GlobalAveragePooling2D(),
        layers.Dropout(0.5),
        layers.Dense(128, activation="relu"),
        layers.Dropout(0.5),
        layers.Dense(n_classes, activation="softmax"),
    ])
    model.compile(
        optimizer="adam",
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"],
    )
    return model


# =============================================================================
# CALLBACKS
# =============================================================================
def get_callbacks(size):
    """Standard callbacks."""
    return [
        callbacks.ModelCheckpoint(
            f"best_model_{size}.keras",
            monitor="val_loss",
            save_best_only=True,
            verbose=1,
        ),
        callbacks.EarlyStopping(
            monitor="val_loss",
            patience=8,
            restore_best_weights=True,
        ),
        callbacks.ReduceLROnPlateau(
            monitor="val_loss",
            factor=0.5,
            patience=3,
            verbose=1,
        ),
    ]


# =============================================================================
# TRAIN & EVAL
# =============================================================================
def train_model(x_train, y_train, x_val, y_val, size, n_classes):
    """Train and evaluate model."""
    model = create_model(x_train.shape[1:], n_classes)
    print(f"\n=== {size}x{size} ===")
    model.summary()

    start = time.time()
    history = model.fit(
        x_train, y_train,
        validation_data=(x_val, y_val),
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        callbacks=get_callbacks(size),
        verbose=1,
    )
    train_time = time.time() - start

    loss, acc = model.evaluate(x_val, y_val, verbose=0)
    print(f"Val loss: {loss:.4f}, Acc: {acc:.4f}, Time: {train_time:.1f}s")

    y_prob = model.predict(x_val, batch_size=BATCH_SIZE)
    y_pred = np.argmax(y_prob, axis=1)

    print(classification_report(y_val, y_pred))
    plot_confusion_matrix(y_val, y_pred, size)

    try:
        auc = roc_auc_score(y_val, y_prob, multi_class="ovr")
        print(f"Macro AUC: {auc:.4f}")
    except ValueError:
        pass

    return {"size": size, "acc": acc, "loss": loss, "time": train_time}


def plot_confusion_matrix(y_true, y_pred, size):
    """Plot confusion matrix."""
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
    plt.title(f"Confusion Matrix {size}x{size}")
    plt.ylabel("True")
    plt.xlabel("Pred")
    plt.tight_layout()
    plt.show()


# =============================================================================
# MAIN
# =============================================================================
if __name__ == "__main__":
    labels = load_labels(INFO_CSV)
    n_classes = labels.max() + 1

    results = []
    for size in RESOLUTIONS:
        images = load_images(size)
        x_train, x_val, y_train, y_val = prepare_data(images, labels)
        result = train_model(x_train, y_train, x_val, y_val, size, n_classes)
        results.append(result)

    # Summary
    df = pd.DataFrame(results)
    print("\n=== SUMMARY ===")
    print(df.round(4))
    df.to_csv("results.csv", index=False)
    print("Saved to results.csv")
